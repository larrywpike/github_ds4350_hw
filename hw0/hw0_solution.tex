\documentclass[12pt, fullpage,letterpaper]{article}

\usepackage[margin=1in]{geometry}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{graphicx}

\newcommand{\semester}{Fall 2021}
\newcommand{\assignmentId}{0}
\newcommand{\releaseDate}{24 Aug, 2021}
\newcommand{\dueDate}{11:59pm, 3 Sep, 2021}

\newcommand{\bx}{{\bf x}}
\newcommand{\bw}{{\bf w}}

\title{CS 5350/6350: Machine Learning \semester}
\author{Homework \assignmentId:   Larry Pike, u0342538}

\date{Handed out: \releaseDate\\
  Due: \dueDate}

\begin{document}
\maketitle

\input{../../emacscomm}




\section*{Basic Knowledge Review}
\label{sec:q1}
\begin{enumerate}
\item~[5 points] We use sets to represent events. For example, toss a fair coin $10$ times, and the event can be represented by the set of ``Heads" or ``Tails" after each tossing. Let a specific event $A$ be ``at least one head". Calculate the probability that event $A$ happens, i.e., $p(A)$.\\

Answer: 
%\vspace{5mm} 
A set of 10 items, each item having 1 of 2 possible states, yields   ${ 2^{10} }$ = 1024 possible outcomes.  As there is only one outcome which does not contain a head, the probablity of there being at least one head is ${  (1024-1)/(1024) =  0.99902 }$


%%% Q 2
\item~[10 points] Given two events $A$ and $B$, prove that 
\[ p(A \cup B) \le p(A) + p(B). \]
When does the equality hold?

Answer: 
%\vspace{5mm} 
The rule of addition in probability states\\
\[ p(A \cup B) = p(A) + p(B) +p( A \cap B)     . \]
Since ${ 0 \le p( A \cap B) \le 1 }$, i.e. zero or positive, then \\ ${ p(A \cap B) \le p(A) + p(B)  }$\\

The equality will hold if ${ p(A \cap B) = 0   }$, thus A and B are independent

%%% Q 3 )
\item~[10 points] Let $\{A_1, \ldots, A_n\}$ be a collection of events. Show that
\[
p(\cup_{i=1}^n A_i) \le \sum_{i=1}^n p(A_i).
\]
When does the equality hold? (Hint: induction)
%\item~[5 points] Given three events $A$, $B$ and $C$, show that
%\[
%p(A\cap B\cap C) = p(A|B\cap C)p(B|C)p(C)
%\]


%%% Q 4 )
\item~[20 points]  We use $\EE(\cdot)$ and $\VV(\cdot)$ to denote a random variable's mean (or expectation) and variance, respectively. Given two discrete random variables $X$ and $Y$, where $X \in \{0, 1\}$ and $Y \in \{0,1\}$. The joint probability $p(X,Y)$ is given in as follows:
\begin{table}[h]
        \centering
        \begin{tabular}{ccc}
        \hline\hline
         & $Y=0$ & $Y=1$ \\ \hline
         $X=0$ & $1/10$ & $2/10$ \\ \hline
         $X=1$  & $3/10$ & $4/10$ \\ \hline\hline
        \end{tabular}
        \caption{Joint Probability, ${ p(X \cap Y)}$ }
\end{table}
	
%%% Question 4  a i)
        \begin{enumerate}
            \item~[10 points] Calculate the following distributions and statistics. 
            \begin{enumerate}
            \item the the marginal distributions $p(X)$ and $p(Y)$ \\
            Answer: \\
%\vspace{5mm} 

$p(X=0) = 3/10$ \\ 
$p(X=1) = 7/10$ \\ 
$p(Y=0) = 4/10$ \\ 
$p(Y=1) = 6/10$ \\ 



%%% Question 4  a ii)
            \item the conditional distributions $p(X|Y)$ and $p(Y|X)$
            
            Answer: \\
            %\vspace{5mm} 
$p(X=0|Y=0) = 1/10$ \\            
$p(X=0|Y=1) = 2/10$  \\           
$p(X=1|Y=0) = 3/10$ \\            
$p(X=1|Y=1) = 4/10$  \\   
$p(Y=0|X=0) = 1/10$ \\            
$p(Y=0|X=1) = 3/10$ \\            
$p(Y=1|X=0) = 2/10$ \\            
$p(Y=1|X=1) = 4/10$ \\            

%%% Question 4  a iii)
            \item $\EE(X)$, $\EE(Y)$, $\VV(X)$, $\VV(Y)$ \\
            Answer: \\
%\vspace{5mm}
$\EE(X) = 0 \cdot ( 1/10 + 2/10)  + 1 \cdot ( 3/10 + 4/10) = \textbf{0.7}$\\           
$\EE(Y) = 0 \cdot ( 1/10 + 3/10)  + 1 \cdot ( 2/10 + 4/10) = \textbf{0.6}$\\  \\
$\  \sigma_X^2 = \sum_{i=1}^2   (x_i - E(X ))^2 \cdot p(x_i) = E(X^2) - { E(X)}^2 $ \\
$\EE(X^2) = 0^2 \cdot (1/10 + 2/ 10) + 1^2   \cdot (3/10 + 4/ 10)  = 0.7$\\           
$\sigma_X^2 =  0.7 - (0.7)^2 = \textbf{0.21}$\\   
   
similiarly for Y      \\
$\EE(Y^2) = 0^2 \cdot (1/10 + 3/ 10) + 1^2   \cdot (2/10 + 4/ 10)  = 0.6$\\           
$\sigma_Y^2 =  0.6 - (0.6)^2 = \textbf{0.24}$\\     


%%% Question 4  a iv)
            \item  $\EE(Y|X=0)$, $\EE(Y|X=1)$,  $\VV(Y|X=0)$, $\VV(Y|X=1)$ \\
            Answer: \\
            \textbf{When ${ X=0,}$} \\  
            $\EE(Y|X=0)= \frac{ 0 \cdot 1/10 +  1 \cdot 2/10}{1/10 + 2/10} = \textbf{2/3}$\\
            ${ p(Y=0|X=0) = \frac{1/10}{1/10 + 2/10}  = 1/3  }$\\
            ${ p(Y=1|X=0) = \frac{2/10}{1/10 + 2/10}  = 2/3  }$\\
            $\  \sigma_{Y|X=0 }^2 = (y_0 - E(Y=0))^2 \cdot p(Y=0) + (y_1 - E(Y=1))^2 \cdot p(Y=1)$ \\
            ${ \sigma_{Y|X=0 }^2 = (0-2/3) ^2 \cdot 1/3 + (1-2/3)^2 \cdot 2/3  }$\\
           ${ \sigma_{Y|X=0 }^2 = 11/27 = \textbf{0.407} }$ \\
           
            When ${ X=1,}$  \\
             $\EE(Y|X=1)= \frac{ 0 \cdot 3/10 +  1 \cdot 4/10}{3/10 + 4/10} = \textbf{4/7}$\\	
            ${ p(Y=0|X=1) = \frac{3/10}{3/10 + 4/10}  = 3/7  }$\\
            ${ p(Y=1|X=1) = \frac{4/10}{3/10 + 4/10}  = 4/7  }$\\
             $\  \sigma_{Y|X=1 }^2 = (y_0 - E(Y=0))^2 \cdot p(Y=0) + (y_1 - E(Y=1))^2 \cdot p(Y=1)$ \\
			 ${ \sigma_{Y|X=1 }^2 = (0-4/7) ^2 \cdot 3/7 + (1-4/7)^2 \cdot 4/7  }$\\
			 ${ \sigma_{Y|X=0 }^2 = 11/27 = \textbf{0.2449} }$ \\
 
            
%%% Question 4  a v)          
            \item  the covariance between $X$ and $Y$
            \end{enumerate}

%%% Question 4  b)            
            \item~[5 points] Are $X$ and $Y$ independent? Why? \\
             Answer: \\
            \textbf{ No.} \\
            Independence requires ${ p(A \cap B) = p(A) \cdot p(B)     .}$\\
            But ${ p((X=0) \cap (Y=0)) =  0.1 }$\\
            ${ p(X=0) = 0.2 }$\\
            ${ p(Y=0) = 0.4 }$\\
            ${ p((X=0) \cdot p(Y=0)) =  0.06 }$\\
            so ${ p(X \cap Y) \ne p(X) \cdot p(Y)}$
            
%%% Question 4  c)                
            \item~[5 points] When $X$ is not assigned a specific value, are $\EE(Y|X)$ and $\VV(Y|X)$ still constant? Why?\\
            
            
        \end{enumerate}
    
%%% Question 5)       
\item~[10 points] Assume a random variable $X$ follows a standard normal distribution, \ie $X \sim \N(X|0, 1)$. Let $Y = e^X$. Calculate the mean and variance of $Y$.
\begin{enumerate}
	\item $\EE(Y)$
	\item $\VV(Y)$
\end{enumerate}

%%% Question 6)     
\item  Given two random variables $X$ and $Y$, show that 

%%% Question 6 a)     
\begin{enumerate}
\item~[20 points] $\EE(\EE(Y|X)) = \EE(Y)$

%%% Question 6 b)  
\item~[\textbf{Bonus question} 20 points]
$\VV(Y) = \EE(\VV(Y|X)) + \VV(\EE(Y|X))$
\end{enumerate}
(Hints: using definition.)

%\item~[20 points]  Let us go back to the coin tossing example. Suppose we toss a coin for $n$ times, \textit{independently}. Each toss we have $\frac{1}{2}$ chance to obtain the head. Let us denote the total number of heads by $c(n)$. Derive the following statistics. You don't need to give the numerical values. You only need to provide the formula.
%\begin{enumerate}
%\item $\EE(c(1))$, $\VV(c(1))$
%\item $\EE(c(10))$, $\VV(c(10))$
%\item $\EE(c(n))$, $\VV(c(n))$
%\end{enumerate} 
%What can you conclude from comparing the expectations and variances with different choices of $n$?  


%%% Question 7)     
\item~[15 points] Given a logistic function, $f(\x) = 1/(1+\exp(-\a^\top \x))$ ($\x$ is a vector), derive/calculate the following gradients and Hessian matrices.  
\begin{enumerate}
\item $\nabla f(\x)$
\item $\nabla^2 f(\x)$
\item $\nabla f(\x)$ when $\a = [1,1,1,1,1]^\top$ and $\x = [0,0,0,0,0]^\top$
\item $\nabla^2 f(\x)$  when $\a = [1,1,1,1,1]^\top$ and $\x = [0,0,0,0,0]^\top$
\end{enumerate}
Note that $0 \le f(\x) \le 1$.


%%% Question 8)    
\item~[10 points] Show that $g(x) = -\log(f(\x))$ where $f(\x)$ is a logistic function defined as above, is convex. 


\end{enumerate}


\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
